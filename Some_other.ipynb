{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5nahMu8wExO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a853a959",
        "outputId": "831f7584-5185-4273-c9e7-624686a7fd88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data loaded. Shape: (10002, 786)\n",
            "Validation data loaded. Shape: (2499, 786)\n",
            "\n",
            "First 5 rows of training data:\n",
            "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x20  28x21  \\\n",
            "0      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "1      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "3      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
            "\n",
            "   28x22  28x23  28x24  28x25  28x26  28x27  28x28  even  \n",
            "0      0      0      0      0      0      0      0     1  \n",
            "1      0      0      0      0      0      0      0     0  \n",
            "2      0      0      0      0      0      0      0     1  \n",
            "3      0      0      0      0      0      0      0     0  \n",
            "4      0      0      0      0      0      0      0     1  \n",
            "\n",
            "[5 rows x 786 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the training and validation datasets\n",
        "df_train = pd.read_csv('/content/MNIST_train.csv')\n",
        "df_val = pd.read_csv('/content/MNIST_validation.csv')\n",
        "\n",
        "print(\"Training data loaded. Shape:\", df_train.shape)\n",
        "print(\"Validation data loaded. Shape:\", df_val.shape)\n",
        "\n",
        "# Display the first few rows of the training dataframe\n",
        "print(\"\\nFirst 5 rows of training data:\")\n",
        "print(df_train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af6cad5d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dataframes are loaded, the next step is to separate features from labels, normalize the pixel values, and ensure the correct data types as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f64b985",
        "outputId": "726ce0fa-5cf1-43ac-8006-4f261d9fdbc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features and labels separated and normalized.\n",
            "X_train shape: (10002, 784) Dtype: float32\n",
            "y_train shape: (10002,) Dtype: int32\n",
            "X_val shape: (2499, 784) Dtype: float32\n",
            "y_val shape: (2499,) Dtype: int32\n"
          ]
        }
      ],
      "source": [
        "X_train = df_train.drop(columns=['label', 'even']).values\n",
        "y_train = df_train['label'].values\n",
        "\n",
        "X_val = df_val.drop(columns=['label', 'even']).values\n",
        "y_val = df_val['label'].values\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "# Convert feature arrays to float and label arrays to int\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "y_train = y_train.astype('int32')\n",
        "y_val = y_val.astype('int32')\n",
        "\n",
        "print(\"Features and labels separated and normalized.\")\n",
        "print(\"X_train shape:\", X_train.shape, \"Dtype:\", X_train.dtype)\n",
        "print(\"y_train shape:\", y_train.shape, \"Dtype:\", y_train.dtype)\n",
        "print(\"X_val shape:\", X_val.shape, \"Dtype:\", X_val.dtype)\n",
        "print(\"y_val shape:\", y_val.shape, \"Dtype:\", y_val.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7cf847e"
      },
      "source": [
        "## Softmax Regression (Multiclass Logistic Regression)\n",
        "\n",
        "Implement Softmax Regression for multiclass classification, including softmax activation, multiclass cross-entropy loss, gradient descent optimization, prediction, accuracy evaluation, and training time recording.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70e4c89d"
      },
      "source": [
        "**Reasoning**:\n",
        "I will start by defining the `SoftmaxRegression` class and implement its constructor to initialize weights and biases, which are essential for the model. Then I will define the softmax activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33893cf8",
        "outputId": "34a5c1fb-ce8b-4e5c-a25c-367409ee123f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SoftmaxRegression class structure initialized with softmax and cross-entropy loss functions.\n"
          ]
        }
      ],
      "source": [
        "class SoftmaxRegression:\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        # Initialize weights with small random values\n",
        "        self.weights = np.random.randn(num_features, num_classes) * 0.01\n",
        "        # Initialize biases to zeros\n",
        "        self.biases = np.zeros((1, num_classes))\n",
        "\n",
        "    def softmax(self, z):\n",
        "        # Subtract the max for numerical stability\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, y_pred_prob, y_true_one_hot):\n",
        "        # Avoid log(0) by clipping probabilities\n",
        "        epsilon = 1e-9\n",
        "        y_pred_prob = np.clip(y_pred_prob, epsilon, 1. - epsilon)\n",
        "        loss = -np.sum(y_true_one_hot * np.log(y_pred_prob)) / y_pred_prob.shape[0]\n",
        "        return loss\n",
        "\n",
        "print(\"SoftmaxRegression class structure initialized with softmax and cross-entropy loss functions.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c702a3e3",
        "outputId": "ef68a1f4-76eb-47fb-8c66-151bc82644d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Softmax Regression training...\n",
            "Training finished in 20.73 seconds.\n",
            "Softmax Regression Validation Accuracy: 0.9012\n",
            "Softmax Regression Training Time: 20.73 seconds.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "class SoftmaxRegression:\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        self.num_features = num_features\n",
        "        self.num_classes = num_classes\n",
        "        self.weights = np.random.randn(num_features, num_classes) * 0.01\n",
        "        self.biases = np.zeros((1, num_classes))\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def cross_entropy_loss(self, y_pred_prob, y_true_one_hot):\n",
        "        epsilon = 1e-9\n",
        "        y_pred_prob = np.clip(y_pred_prob, epsilon, 1. - epsilon)\n",
        "        loss = -np.sum(y_true_one_hot * np.log(y_pred_prob)) / y_pred_prob.shape[0]\n",
        "        return loss\n",
        "\n",
        "    def compute_gradients(self, X, y_true_one_hot, y_pred_prob):\n",
        "        # Gradient of loss with respect to scores (logits)\n",
        "        grad_logits = y_pred_prob - y_true_one_hot\n",
        "        # Gradient of loss with respect to weights\n",
        "        grad_weights = np.dot(X.T, grad_logits) / X.shape[0]\n",
        "        # Gradient of loss with respect to biases\n",
        "        grad_biases = np.sum(grad_logits, axis=0, keepdims=True) / X.shape[0]\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.1, epochs=100):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Convert y to one-hot encoding\n",
        "        y_one_hot = np.zeros((y.size, self.num_classes))\n",
        "        y_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Compute scores (logits)\n",
        "            z = np.dot(X, self.weights) + self.biases\n",
        "            # Apply softmax to get probabilities\n",
        "            y_pred_prob = self.softmax(z)\n",
        "            # Compute loss\n",
        "            loss = self.cross_entropy_loss(y_pred_prob, y_one_hot)\n",
        "\n",
        "            # Compute gradients\n",
        "            grad_weights, grad_biases = self.compute_gradients(X, y_one_hot, y_pred_prob)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights -= learning_rate * grad_weights\n",
        "            self.biases -= learning_rate * grad_biases\n",
        "\n",
        "            # Optional: print loss every few epochs\n",
        "            # if (epoch + 1) % 10 == 0:\n",
        "            #     print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        print(f\"Training finished in {self.training_time:.2f} seconds.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute scores (logits)\n",
        "        z = np.dot(X, self.weights) + self.biases\n",
        "        # Apply softmax to get probabilities\n",
        "        y_pred_prob = self.softmax(z)\n",
        "        # Return the class with the highest probability\n",
        "        return np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    accuracy = np.mean(y_true == y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# Create an instance of the SoftmaxRegression model\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "softmax_model = SoftmaxRegression(num_features, num_classes)\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting Softmax Regression training...\")\n",
        "softmax_model.fit(X_train, y_train, learning_rate=0.5, epochs=200)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred_val = softmax_model.predict(X_val)\n",
        "\n",
        "# Evaluate the model\n",
        "softmax_accuracy = evaluate(y_val, y_pred_val)\n",
        "print(f\"Softmax Regression Validation Accuracy: {softmax_accuracy:.4f}\")\n",
        "print(f\"Softmax Regression Training Time: {softmax_model.training_time:.2f} seconds.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df8fb736"
      },
      "source": [
        "## Perceptron (Multiclass OVR)\n",
        "\n",
        "Implement the Perceptron algorithm using the Multiclass One-vs-Rest strategy, train 10 separate perceptron models, implement the perceptron update rule without activation, predict by choosing the class with the highest output, and evaluate accuracy and record training time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed64acb2",
        "outputId": "61df7432-1249-4ec4-8af0-7524faf0acd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron class defined with __init__, fit, and predict methods.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, num_features):\n",
        "        # Initialize weights to zeros (or small random values)\n",
        "        self.weights = np.zeros(num_features)\n",
        "        # Initialize bias to zero\n",
        "        self.bias = 0\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.01, epochs=100):\n",
        "        # Perceptron update rule\n",
        "        for _ in range(epochs):\n",
        "            for i in range(X.shape[0]):\n",
        "                # Calculate the raw score (without activation function)\n",
        "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
        "                # If misclassified, update weights and bias\n",
        "                # The target labels for OVR will be +1 or -1\n",
        "                if y[i] * linear_output <= 0: # Misclassification condition (y_true * y_pred <= 0)\n",
        "                    self.weights += learning_rate * y[i] * X[i]\n",
        "                    self.bias += learning_rate * y[i]\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Return the raw scores\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "print(\"Perceptron class defined with __init__, fit, and predict methods.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e26bc75c"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `Perceptron` class is defined, I will implement the Multiclass One-vs-Rest strategy by training 10 separate perceptron models for each digit (0-9) and storing them. I will also measure the training time and then proceed with prediction and evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3da3232",
        "outputId": "9222db57-6b30-4361-ddae-f867f40d3c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Perceptron OVR training...\n",
            "Training Perceptron for digit 0...\n",
            "Training Perceptron for digit 1...\n",
            "Training Perceptron for digit 2...\n",
            "Training Perceptron for digit 3...\n",
            "Training Perceptron for digit 4...\n",
            "Training Perceptron for digit 5...\n",
            "Training Perceptron for digit 6...\n",
            "Training Perceptron for digit 7...\n",
            "Training Perceptron for digit 8...\n",
            "Training Perceptron for digit 9...\n",
            "Perceptron OVR training finished in 10.49 seconds.\n",
            "Making predictions on the validation set...\n",
            "\n",
            "Multiclass Perceptron OVR Validation Accuracy: 0.8531\n",
            "Multiclass Perceptron OVR Training Time: 10.49 seconds.\n"
          ]
        }
      ],
      "source": [
        "num_features = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# 2. Implement the Multiclass One-vs-Rest (OVR) strategy\n",
        "perceptron_models = []\n",
        "\n",
        "print(\"Starting Perceptron OVR training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for c in range(num_classes):\n",
        "    print(f\"Training Perceptron for digit {c}...\")\n",
        "    # Create binary training labels: +1 for current class, -1 for others\n",
        "    y_train_binary = np.where(y_train == c, 1, -1)\n",
        "\n",
        "    # Create an instance of the Perceptron class\n",
        "    perceptron_model = Perceptron(num_features)\n",
        "\n",
        "    # Train this perceptron model\n",
        "    perceptron_model.fit(X_train, y_train_binary, learning_rate=0.1, epochs=20)\n",
        "\n",
        "    # Store the trained perceptron model\n",
        "    perceptron_models.append(perceptron_model)\n",
        "\n",
        "perceptron_training_time = time.time() - start_time\n",
        "print(f\"Perceptron OVR training finished in {perceptron_training_time:.2f} seconds.\")\n",
        "\n",
        "# 3. Make predictions on the validation set (X_val)\n",
        "y_pred_val_perceptron = np.zeros(X_val.shape[0])\n",
        "\n",
        "print(\"Making predictions on the validation set...\")\n",
        "for i in range(X_val.shape[0]):\n",
        "    scores = []\n",
        "    for model in perceptron_models:\n",
        "        scores.append(model.predict(X_val[i].reshape(1, -1))[0]) # Get raw score for current sample\n",
        "\n",
        "    # The predicted class is the digit corresponding to the model with the highest score\n",
        "    y_pred_val_perceptron[i] = np.argmax(scores)\n",
        "\n",
        "# 4. Evaluate the model's performance\n",
        "perceptron_accuracy = np.mean(y_val == y_pred_val_perceptron)\n",
        "\n",
        "print(f\"\\nMulticlass Perceptron OVR Validation Accuracy: {perceptron_accuracy:.4f}\")\n",
        "print(f\"Multiclass Perceptron OVR Training Time: {perceptron_training_time:.2f} seconds.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79d5b61"
      },
      "source": [
        "## Gaussian Naive Bayes\n",
        "\n",
        "Implement Gaussian Naive Bayes classifier. For each class (digit), calculate the mean and variance for each pixel from the training data. Use these statistics to compute the Gaussian probability density function for each pixel. Predict the class that maximizes the posterior probability (using log-probabilities to avoid underflow). Evaluate accuracy and record training time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "127a1a5c",
        "outputId": "ec125d1c-03fc-4bc3-e1d8-73fdb7eb4fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Gaussian Naive Bayes training...\n",
            "Gaussian Naive Bayes training finished in 0.10 seconds.\n",
            "Calculated means, variances, and priors for each class.\n",
            "Shape of class_means: (10, 784)\n",
            "Shape of class_variances: (10, 784)\n",
            "Shape of class_priors: (10,)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Initialize lists to store parameters for each class\n",
        "class_means = []\n",
        "class_variances = []\n",
        "class_priors = []\n",
        "\n",
        "num_classes = len(np.unique(y_train))\n",
        "num_features = X_train.shape[1]\n",
        "\n",
        "# Small value to avoid division by zero and log(0)\n",
        "epsilon = 1e-6\n",
        "\n",
        "print(\"Starting Gaussian Naive Bayes training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for c in range(num_classes):\n",
        "    # Filter data for the current class\n",
        "    X_c = X_train[y_train == c]\n",
        "\n",
        "    # Calculate mean and variance for each feature for the current class\n",
        "    mean_c = np.mean(X_c, axis=0)\n",
        "    variance_c = np.var(X_c, axis=0) + epsilon # Add epsilon for numerical stability\n",
        "\n",
        "    class_means.append(mean_c)\n",
        "    class_variances.append(variance_c)\n",
        "\n",
        "    # Calculate prior probability for the current class\n",
        "    prior_c = len(X_c) / len(X_train)\n",
        "    class_priors.append(prior_c)\n",
        "\n",
        "# Convert to numpy arrays for easier manipulation\n",
        "class_means = np.array(class_means)\n",
        "class_variances = np.array(class_variances)\n",
        "class_priors = np.array(class_priors)\n",
        "\n",
        "gnb_training_time = time.time() - start_time\n",
        "print(f\"Gaussian Naive Bayes training finished in {gnb_training_time:.2f} seconds.\")\n",
        "\n",
        "print(\"Calculated means, variances, and priors for each class.\")\n",
        "print(\"Shape of class_means:\", class_means.shape)\n",
        "print(\"Shape of class_variances:\", class_variances.shape)\n",
        "print(\"Shape of class_priors:\", class_priors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b61a3dc7"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the means, variances, and priors have been calculated for each class, I will define a helper function to compute the Gaussian log-Probability Density Function (log-PDF). This function will be crucial for calculating the log-likelihoods in the prediction step, ensuring numerical stability by working with log-probabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cfb5d82",
        "outputId": "7b4d848d-e96d-4009-8b9b-5da53b55c7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gaussian log-PDF function defined.\n"
          ]
        }
      ],
      "source": [
        "def gaussian_log_pdf(X, mean, variance):\n",
        "    # Using log-probabilities for numerical stability\n",
        "    # log(1 / sqrt(2 * pi * variance)) - (x - mean)^2 / (2 * variance)\n",
        "    log_denominator = 0.5 * np.log(2 * np.pi * variance)\n",
        "    numerator = (X - mean)**2\n",
        "    log_likelihood = -0.5 * (numerator / variance) - log_denominator\n",
        "    return log_likelihood\n",
        "\n",
        "print(\"Gaussian log-PDF function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a993a299",
        "outputId": "3855f6ce-5234-4b72-c00d-c854017c552e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making predictions on the validation set using Gaussian Naive Bayes...\n",
            "Gaussian Naive Bayes Validation Accuracy: 0.6355\n",
            "Gaussian Naive Bayes Training Time: 0.10 seconds.\n",
            "Gaussian Naive Bayes Prediction Time: 0.52 seconds.\n"
          ]
        }
      ],
      "source": [
        "def predict_gnb(X):\n",
        "    predictions = np.zeros(X.shape[0])\n",
        "    for i, sample in enumerate(X):\n",
        "        log_posteriors = []\n",
        "        for c in range(num_classes):\n",
        "            # Calculate log-prior\n",
        "            log_prior = np.log(class_priors[c])\n",
        "\n",
        "            # Calculate log-likelihood for the sample given the class\n",
        "            # Sum of log-PDFs for each feature\n",
        "            log_likelihood_per_feature = gaussian_log_pdf(sample, class_means[c], class_variances[c])\n",
        "            log_likelihood = np.sum(log_likelihood_per_feature)\n",
        "\n",
        "            # Calculate log-posterior\n",
        "            log_posterior = log_prior + log_likelihood\n",
        "            log_posteriors.append(log_posterior)\n",
        "\n",
        "        # Predict the class with the maximum log-posterior probability\n",
        "        predictions[i] = np.argmax(log_posteriors)\n",
        "    return predictions\n",
        "\n",
        "print(\"Making predictions on the validation set using Gaussian Naive Bayes...\")\n",
        "prediction_start_time = time.time()\n",
        "y_pred_val_gnb = predict_gnb(X_val)\n",
        "gnb_prediction_time = time.time() - prediction_start_time\n",
        "\n",
        "# Evaluate the model\n",
        "gnb_accuracy = np.mean(y_val == y_pred_val_gnb)\n",
        "\n",
        "print(f\"Gaussian Naive Bayes Validation Accuracy: {gnb_accuracy:.4f}\")\n",
        "print(f\"Gaussian Naive Bayes Training Time: {gnb_training_time:.2f} seconds.\")\n",
        "print(f\"Gaussian Naive Bayes Prediction Time: {gnb_prediction_time:.2f} seconds.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "237d19f5"
      },
      "source": [
        "## k-Nearest Neighbors (k-NN)\n",
        "\n",
        "Implement the k-Nearest Neighbors algorithm from scratch, including distance calculation, finding nearest neighbors, and majority voting for prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ab04b95",
        "outputId": "863455bd-b404-4169-df0c-1b52cd4e58a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNearestNeighbors class defined with __init__ and fit methods.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class KNearestNeighbors:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(\"KNN model fitted with training data.\")\n",
        "\n",
        "print(\"KNearestNeighbors class defined with __init__ and fit methods.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7380db31"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `KNearestNeighbors` class has its `__init__` and `fit` methods, I will implement the `euclidean_distance`, `find_nearest_neighbors`, and `predict` methods within the class to handle distance calculation, neighbor identification, and majority voting for predictions, completing the implementation of the KNN algorithm as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "179bf3a4",
        "outputId": "8b2453dd-629d-4eac-9504-bab2514bbabb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting k-NN training (fitting data)...\n",
            "KNN model fitted with training data.\n",
            "k-NN training (fitting) finished in 0.00 seconds.\n",
            "Making predictions on the validation set using k-NN...\n",
            "\n",
            "k-Nearest Neighbors Validation Accuracy: 0.9504\n",
            "k-Nearest Neighbors Training Time: 0.00 seconds.\n",
            "k-Nearest Neighbors Prediction Time: 273.04 seconds.\n"
          ]
        }
      ],
      "source": [
        "class KNearestNeighbors:\n",
        "    def __init__(self, n_neighbors=5):\n",
        "        self.n_neighbors = n_neighbors\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        print(\"KNN model fitted with training data.\")\n",
        "\n",
        "    def euclidean_distance(self, x1, x2):\n",
        "        # Calculate Euclidean distance between two points\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def find_nearest_neighbors(self, X_test_sample):\n",
        "        distances = [self.euclidean_distance(X_test_sample, x_train) for x_train in self.X_train]\n",
        "        # Get the indices of the k smallest distances\n",
        "        k_nearest_indices = np.argsort(distances)[:self.n_neighbors]\n",
        "        # Return the labels of the k nearest neighbors\n",
        "        return [self.y_train[i] for i in k_nearest_indices]\n",
        "\n",
        "    def predict_single_sample(self, X_test_sample):\n",
        "        neighbors_labels = self.find_nearest_neighbors(X_test_sample)\n",
        "        # Perform majority voting\n",
        "        unique_labels, counts = np.unique(neighbors_labels, return_counts=True)\n",
        "        return unique_labels[np.argmax(counts)]\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = [self.predict_single_sample(x) for x in X_test]\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "# Initialize and fit the model\n",
        "knn_model = KNearestNeighbors(n_neighbors=5) # You can change k here\n",
        "print(\"Starting k-NN training (fitting data)...\")\n",
        "knn_training_start_time = time.time()\n",
        "knn_model.fit(X_train, y_train)\n",
        "knn_training_time = time.time() - knn_training_start_time\n",
        "print(f\"k-NN training (fitting) finished in {knn_training_time:.2f} seconds.\")\n",
        "\n",
        "# Make predictions on the validation set\n",
        "print(\"Making predictions on the validation set using k-NN...\")\n",
        "knn_prediction_start_time = time.time()\n",
        "y_pred_val_knn = knn_model.predict(X_val)\n",
        "knn_prediction_time = time.time() - knn_prediction_start_time\n",
        "\n",
        "# Evaluate the model\n",
        "knn_accuracy = np.mean(y_val == y_pred_val_knn)\n",
        "\n",
        "print(f\"\\nk-Nearest Neighbors Validation Accuracy: {knn_accuracy:.4f}\")\n",
        "print(f\"k-Nearest Neighbors Training Time: {knn_training_time:.2f} seconds.\")\n",
        "print(f\"k-Nearest Neighbors Prediction Time: {knn_prediction_time:.2f} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637eb884"
      },
      "source": [
        "## Linear SVM (Gradient Descent)\n",
        "\n",
        "Implement the Linear SVM model, including its initialization and the hinge loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0edc4f91",
        "outputId": "47684dcb-8b98-4082-df2c-4adef8bd43a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LinearSVM class initialized with __init__ and hinge_loss methods.\n"
          ]
        }
      ],
      "source": [
        "class LinearSVM:\n",
        "    def __init__(self, num_features):\n",
        "        # Initialize weights with small random values to break symmetry\n",
        "        self.weights = np.random.randn(num_features) * 0.01\n",
        "        # Initialize bias to zero\n",
        "        self.bias = 0\n",
        "\n",
        "    def hinge_loss(self, y_true, y_pred_score):\n",
        "        # y_true should be +1 or -1\n",
        "        # y_pred_score is the raw output from the SVM (w.x + b)\n",
        "        return np.maximum(0, 1 - y_true * y_pred_score)\n",
        "\n",
        "print(\"LinearSVM class initialized with __init__ and hinge_loss methods.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beb3cced"
      },
      "source": [
        "## Linear SVM (Gradient Descent)\n",
        "\n",
        "Implement the `fit` method to optimize the Linear SVM using gradient descent and the `predict` method for classification. Evaluate the model's accuracy on the validation set and record its training time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2da08ae7",
        "outputId": "794d7c50-47e2-4606-d835-3c9466c4dd4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Linear SVM OVR training...\n",
            "Training Linear SVM for digit 0...\n",
            "Training Linear SVM for digit 1...\n",
            "Training Linear SVM for digit 2...\n",
            "Training Linear SVM for digit 3...\n",
            "Training Linear SVM for digit 4...\n",
            "Training Linear SVM for digit 5...\n",
            "Training Linear SVM for digit 6...\n",
            "Training Linear SVM for digit 7...\n",
            "Training Linear SVM for digit 8...\n",
            "Training Linear SVM for digit 9...\n",
            "Linear SVM OVR training finished in 48.07 seconds.\n",
            "Making predictions on the validation set using Linear SVM OVR...\n",
            "\n",
            "Linear SVM (Gradient Descent) OVR Validation Accuracy: 0.8980\n",
            "Linear SVM (Gradient Descent) OVR Training Time: 48.07 seconds.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "class LinearSVM:\n",
        "    def __init__(self, num_features):\n",
        "        # Initialize weights with small random values\n",
        "        self.weights = np.random.randn(num_features) * 0.01\n",
        "        # Initialize bias to zero\n",
        "        self.bias = 0\n",
        "        self.training_time = 0\n",
        "\n",
        "    def hinge_loss(self, y_true, y_pred_score):\n",
        "        # y_true should be +1 or -1\n",
        "        # y_pred_score is the raw output from the SVM (w.x + b)\n",
        "        return np.maximum(0, 1 - y_true * y_pred_score)\n",
        "\n",
        "    def fit(self, X, y_binary, learning_rate=0.001, epochs=100, C=0.01):\n",
        "        # y_binary labels should be +1 or -1\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Iterate through each training sample\n",
        "            for i in range(X.shape[0]):\n",
        "                x_i = X[i]\n",
        "                y_i = y_binary[i]\n",
        "\n",
        "                # Calculate the score (decision function output)\n",
        "                score = np.dot(x_i, self.weights) + self.bias\n",
        "\n",
        "                # Update weights and bias based on the hinge loss subgradient\n",
        "                if y_i * score < 1:  # Misclassified or within the margin\n",
        "                    self.weights = self.weights + learning_rate * (y_i * x_i - 2 * C * self.weights)\n",
        "                    self.bias = self.bias + learning_rate * y_i\n",
        "                else:  # Correctly classified and outside the margin\n",
        "                    self.weights = self.weights - learning_rate * 2 * C * self.weights\n",
        "                    # Bias remains unchanged in this case\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Compute raw scores (decision function output)\n",
        "        scores = np.dot(X, self.weights) + self.bias\n",
        "        # Return class predictions: +1 if score > 0, -1 otherwise\n",
        "        return np.where(scores > 0, 1, -1)\n",
        "\n",
        "# Helper function to evaluate accuracy (re-using the one defined in softmax section)\n",
        "# def evaluate(y_true, y_pred):\n",
        "#     accuracy = np.mean(y_true == y_pred)\n",
        "#     return accuracy\n",
        "\n",
        "# Multiclass One-vs-Rest (OVR) strategy for Linear SVM\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "svm_models_ovr = []\n",
        "overall_svm_training_start_time = time.time()\n",
        "\n",
        "print(\"Starting Linear SVM OVR training...\")\n",
        "for c in range(num_classes):\n",
        "    print(f\"Training Linear SVM for digit {c}...\")\n",
        "    # Convert original labels to binary: +1 for current class, -1 for all others\n",
        "    y_train_binary_ovr = np.where(y_train == c, 1, -1)\n",
        "\n",
        "    # Create an instance of LinearSVM for the current class\n",
        "    svm_model = LinearSVM(num_features)\n",
        "\n",
        "    # Train the SVM model with specified hyperparameters\n",
        "    # Using a small C for L2 regularization and a small learning rate for stability\n",
        "    svm_model.fit(X_train, y_train_binary_ovr, learning_rate=0.0001, epochs=50, C=0.001)\n",
        "    svm_models_ovr.append(svm_model)\n",
        "\n",
        "linear_svm_ovr_training_time = time.time() - overall_svm_training_start_time\n",
        "print(f\"Linear SVM OVR training finished in {linear_svm_ovr_training_time:.2f} seconds.\")\n",
        "\n",
        "# Make predictions on the validation set using the OVR strategy\n",
        "print(\"Making predictions on the validation set using Linear SVM OVR...\")\n",
        "\n",
        "# Store raw scores from each binary SVM for each validation sample\n",
        "prediction_scores = np.zeros((X_val.shape[0], num_classes))\n",
        "\n",
        "for c, model in enumerate(svm_models_ovr):\n",
        "    # The raw decision function output (w.x + b) is used for OVR prediction\n",
        "    scores_for_class_c = np.dot(X_val, model.weights) + model.bias\n",
        "    prediction_scores[:, c] = scores_for_class_c\n",
        "\n",
        "# The predicted class for each sample is the one whose corresponding SVM yielded the highest score\n",
        "y_pred_val_svm_ovr = np.argmax(prediction_scores, axis=1)\n",
        "\n",
        "# Evaluate the model's accuracy on the validation set\n",
        "linear_svm_ovr_accuracy = np.mean(y_val == y_pred_val_svm_ovr) # Using numpy mean for accuracy\n",
        "\n",
        "print(f\"\\nLinear SVM (Gradient Descent) OVR Validation Accuracy: {linear_svm_ovr_accuracy:.4f}\")\n",
        "print(f\"Linear SVM (Gradient Descent) OVR Training Time: {linear_svm_ovr_training_time:.2f} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e018d642"
      },
      "source": [
        "## k-Means Clustering + Label Mapping\n",
        "\n",
        "Implement the k-Means clustering algorithm, including initialization of centroids, the iterative assignment of points to clusters, and updating of centroids until convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8bc8872",
        "outputId": "afb9a572-11e7-47f7-ee5d-51b2f341a568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMeans class defined with __init__ and fit methods.\n"
          ]
        }
      ],
      "source": [
        "class KMeans:\n",
        "    def __init__(self, n_clusters=10, max_iter=300, random_state=None):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "        self.centroids = None\n",
        "        self.labels = None\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X):\n",
        "        if self.random_state:\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "        # 1. Initialize centroids randomly from the data points\n",
        "        # Randomly select n_clusters indices from X\n",
        "        initial_indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)\n",
        "        self.centroids = X[initial_indices]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # 2. Assign each data point to the closest centroid\n",
        "            distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "            self.labels = np.argmin(distances, axis=0)\n",
        "\n",
        "            # 3. Update centroids\n",
        "            new_centroids = np.array([X[self.labels == k].mean(axis=0) if np.any(self.labels == k) else self.centroids[k]\n",
        "                                      for k in range(self.n_clusters)])\n",
        "\n",
        "            # Check for convergence\n",
        "            if np.allclose(self.centroids, new_centroids, atol=1e-4):\n",
        "                print(f\"K-Means converged after {i+1} iterations.\")\n",
        "                break\n",
        "\n",
        "            self.centroids = new_centroids\n",
        "        else:\n",
        "            print(f\"K-Means did not converge after {self.max_iter} iterations.\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Assign each data point to the closest centroid\n",
        "        distances = np.sqrt(((X - self.centroids[:, np.newaxis])**2).sum(axis=2))\n",
        "        return np.argmin(distances, axis=0)\n",
        "\n",
        "print(\"KMeans class defined with __init__ and fit methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c327d7b",
        "outputId": "ba423670-0a4a-492b-a1ce-baf36120d06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting k-Means training...\n",
            "K-Means converged after 44 iterations.\n",
            "k-Means training finished in 10.48 seconds.\n",
            "Cluster to label mapping created: [4 9 1 8 3 1 7 2 6 0]\n",
            "Making predictions on the validation set using k-Means...\n",
            "\n",
            "k-Means Clustering + Label Mapping Validation Accuracy: 0.5818\n",
            "k-Means Clustering + Label Mapping Training Time: 10.48 seconds.\n",
            "k-Means Clustering + Label Mapping Prediction Time: 0.05 seconds.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Initialize and fit the KMeans model\n",
        "kmeans_model = KMeans(n_clusters=num_classes, random_state=42) # num_classes is 10 for MNIST\n",
        "\n",
        "print(\"Starting k-Means training...\")\n",
        "kmeans_training_start_time = time.time()\n",
        "kmeans_model.fit(X_train)\n",
        "kmeans_training_time = time.time() - kmeans_training_start_time\n",
        "print(f\"k-Means training finished in {kmeans_training_time:.2f} seconds.\")\n",
        "\n",
        "# Map clusters to true labels\n",
        "# For each cluster, find the most frequent true label among its members\n",
        "cluster_to_label_map = np.zeros(num_classes, dtype=int)\n",
        "for i in range(num_classes):\n",
        "    # Get true labels for all data points assigned to cluster i\n",
        "    labels_in_cluster = y_train[kmeans_model.labels == i]\n",
        "    if len(labels_in_cluster) > 0:\n",
        "        # Find the most frequent label in this cluster\n",
        "        unique_labels, counts = np.unique(labels_in_cluster, return_counts=True)\n",
        "        cluster_to_label_map[i] = unique_labels[np.argmax(counts)]\n",
        "    else:\n",
        "        # Handle empty clusters, e.g., assign an arbitrary label or skip\n",
        "        cluster_to_label_map[i] = -1 # Indicate unassigned/empty cluster\n",
        "\n",
        "print(\"Cluster to label mapping created:\", cluster_to_label_map)\n",
        "\n",
        "# Predict on the validation set\n",
        "print(\"Making predictions on the validation set using k-Means...\")\n",
        "kmeans_prediction_start_time = time.time()\n",
        "y_pred_val_kmeans_clusters = kmeans_model.predict(X_val)\n",
        "y_pred_val_kmeans = np.array([cluster_to_label_map[cluster_id] for cluster_id in y_pred_val_kmeans_clusters])\n",
        "kmeans_prediction_time = time.time() - kmeans_prediction_start_time\n",
        "\n",
        "# Evaluate accuracy (ignoring unassigned clusters for evaluation if any)\n",
        "# For simplicity, we assume all clusters are mapped to a valid label here\n",
        "\n",
        "kmeans_accuracy = np.mean(y_val == y_pred_val_kmeans)\n",
        "\n",
        "print(f\"\\nk-Means Clustering + Label Mapping Validation Accuracy: {kmeans_accuracy:.4f}\")\n",
        "print(f\"k-Means Clustering + Label Mapping Training Time: {kmeans_training_time:.2f} seconds.\")\n",
        "print(f\"k-Means Clustering + Label Mapping Prediction Time: {kmeans_prediction_time:.2f} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6HP2V53amUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wngdw1HRamRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1KvDV3DamOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Y9fUhSMamL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPsXHkwOamJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56518e9c"
      },
      "source": [
        "## XGBoost (from Scratch)\n",
        "\n",
        "Implement the core `Node` and `DecisionTree` classes for XGBoost, handling splitting criteria, gain calculation, and leaf value computation based on gradients and Hessians.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a8b8441",
        "outputId": "fdf896b2-9bf2-4542-93a2-faa76002a1db"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature_idx=None, threshold=None, value=None, left=None, right=None, gain=None):\n",
        "        # For decision node\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.gain = gain\n",
        "        # For leaf node\n",
        "        self.value = value\n",
        "\n",
        "print(\"Node class defined for XGBoost Decision Tree.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node class defined for XGBoost Decision Tree.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6a1b803",
        "outputId": "bd158b31-8887-42c7-a57a-bff348939110"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    # (Placeholder for future methods: _find_best_split, _calculate_leaf_value, _grow_tree, fit, predict_one_sample, predict)\n",
        "\n",
        "print(\"DecisionTree class defined with constructor.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class defined with constructor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63127a90",
        "outputId": "feb33921-7e82-49fa-b9fc-62f5b1b6c5b1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        # Gain calculation for XGBoost\n",
        "        # G_L, H_L: sum of gradients and hessians in left child\n",
        "        # G_R, H_R: sum of gradients and hessians in right child\n",
        "        # reg_lambda: L2 regularization term\n",
        "        # reg_alpha: L1 regularization term\n",
        "\n",
        "        # Loss for a node (before/after split) using L1 and L2 regularization\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        # Optionally, add gamma for tree complexity regularization (not explicitly in subtask but common)\n",
        "        # gain -= self.gamma # Tree complexity cost\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Sum of gradients and hessians for the current node\n",
        "        G_node = np.sum(g)\n",
        "        H_node = np.sum(h)\n",
        "\n",
        "        # Check if splitting is even possible (e.g., enough samples)\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            return None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            # Sort unique thresholds for the current feature\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            # If there's only one unique value, no split possible for this feature\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                # Split data based on the current feature and threshold\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                # Ensure minimum samples in each split\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                # Calculate sums of gradients and hessians for left and right children\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                # Calculate gain\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                # Update best split if current_gain is better\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # A split is only considered if it results in a positive gain after complexity cost\n",
        "        # Here, we assume gain includes complexity cost implicitly if self.gamma is used in _calculate_gain\n",
        "        # For now, let's just return if best_gain is significantly positive\n",
        "        if best_gain <= 0: # A common heuristic is to require positive gain\n",
        "            return None, None, None\n",
        "\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    # (Placeholder for future methods: _calculate_leaf_value, _grow_tree, fit, predict_one_sample, predict)\n",
        "\n",
        "print(\"DecisionTree class updated with _calculate_gain and _find_best_split methods.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class updated with _calculate_gain and _find_best_split methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38cd89ba",
        "outputId": "fb765b80-c967-48d8-ce0e-895ff6329d30"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        # Gain calculation for XGBoost\n",
        "        # G_L, H_L: sum of gradients and hessians in left child\n",
        "        # G_R, H_R: sum of gradients and hessians in right child\n",
        "        # reg_lambda: L2 regularization term\n",
        "        # reg_alpha: L1 regularization term\n",
        "\n",
        "        # Loss for a node (before/after split) using L1 and L2 regularization\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        # Optionally, add gamma for tree complexity regularization (not explicitly in subtask but common)\n",
        "        # gain -= self.gamma # Tree complexity cost\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Sum of gradients and hessians for the current node\n",
        "        G_node = np.sum(g)\n",
        "        H_node = np.sum(h)\n",
        "\n",
        "        # Check if splitting is even possible (e.g., enough samples)\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            return None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            # Sort unique thresholds for the current feature\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            # If there's only one unique value, no split possible for this feature\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                # Split data based on the current feature and threshold\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                # Ensure minimum samples in each split\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                # Calculate sums of gradients and hessians for left and right children\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                # Calculate gain\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                # Update best split if current_gain is better\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # A split is only considered if it results in a positive gain after complexity cost\n",
        "        # Here, we assume gain includes complexity cost implicitly if self.gamma is used in _calculate_gain\n",
        "        # For now, let's just return if best_gain is significantly positive\n",
        "        if best_gain <= 0: # A common heuristic is to require positive gain\n",
        "            return None, None, None\n",
        "\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    def _calculate_leaf_value(self, g, h):\n",
        "        # Sum of gradients and hessians for the current leaf node\n",
        "        G = np.sum(g)\n",
        "        H = np.sum(h)\n",
        "\n",
        "        # Apply L1 regularization (reg_alpha)\n",
        "        if self.reg_alpha > 0:\n",
        "            if G > self.reg_alpha:\n",
        "                G -= self.reg_alpha\n",
        "            elif G < -self.reg_alpha:\n",
        "                G += self.reg_alpha\n",
        "            else:\n",
        "                G = 0 # If |G| <= reg_alpha, G effectively becomes 0 due to L1 regularization\n",
        "\n",
        "        # Calculate leaf value (gamma) with L2 regularization (reg_lambda)\n",
        "        if (H + self.reg_lambda) > 0:\n",
        "            leaf_value = -G / (H + self.reg_lambda)\n",
        "        else:\n",
        "            leaf_value = 0 # Avoid division by zero, though H should generally be positive\n",
        "\n",
        "        return leaf_value\n",
        "\n",
        "    # (Placeholder for future methods: _grow_tree, fit, predict_one_sample, predict)\n",
        "\n",
        "print(\"DecisionTree class updated with _calculate_leaf_value method.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class updated with _calculate_leaf_value method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e12162f",
        "outputId": "9e0725d2-763b-4933-d088-f44e536e41ab"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        # Gain calculation for XGBoost\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            return None, None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # Return best_gain as well to check for stopping conditions later\n",
        "        if best_gain <= 0 or np.isinf(best_gain):\n",
        "            return None, None, None, None\n",
        "\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    def _calculate_leaf_value(self, g, h):\n",
        "        G = np.sum(g)\n",
        "        H = np.sum(h)\n",
        "\n",
        "        if self.reg_alpha > 0:\n",
        "            if G > self.reg_alpha:\n",
        "                G -= self.reg_alpha\n",
        "            elif G < -self.reg_alpha:\n",
        "                G += self.reg_alpha\n",
        "            else:\n",
        "                G = 0\n",
        "\n",
        "        if (H + self.reg_lambda) > 0:\n",
        "            leaf_value = -G / (H + self.reg_lambda)\n",
        "        else:\n",
        "            leaf_value = 0\n",
        "\n",
        "        return leaf_value\n",
        "\n",
        "    def _grow_tree(self, X, g, h, depth):\n",
        "        # Stopping conditions\n",
        "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feature_idx, threshold, gain = self._find_best_split(X, g, h)\n",
        "\n",
        "        # If no beneficial split is found, create a leaf node\n",
        "        if feature_idx is None or gain <= 0:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Split data\n",
        "        left_indices = X[:, feature_idx] < threshold\n",
        "        right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "        # Recursively grow left and right subtrees\n",
        "        left_child = self._grow_tree(X[left_indices], g[left_indices], h[left_indices], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_indices], g[right_indices], h[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature_idx=feature_idx, threshold=threshold, left=left_child, right=right_child, gain=gain)\n",
        "\n",
        "    # (Placeholder for future methods: fit, predict_one_sample, predict)\n",
        "\n",
        "print(\"DecisionTree class updated with _grow_tree method.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class updated with _grow_tree method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635fc89a",
        "outputId": "743a1beb-41b0-49a9-ca7d-d87c03484532"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            return None, None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_gain <= 0 or np.isinf(best_gain):\n",
        "            return None, None, None, None\n",
        "\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    def _calculate_leaf_value(self, g, h):\n",
        "        G = np.sum(g)\n",
        "        H = np.sum(h)\n",
        "\n",
        "        if self.reg_alpha > 0:\n",
        "            if G > self.reg_alpha:\n",
        "                G -= self.reg_alpha\n",
        "            elif G < -self.reg_alpha:\n",
        "                G += self.reg_alpha\n",
        "            else:\n",
        "                G = 0\n",
        "\n",
        "        if (H + self.reg_lambda) > 0:\n",
        "            leaf_value = -G / (H + self.reg_lambda)\n",
        "        else:\n",
        "            leaf_value = 0\n",
        "\n",
        "        return leaf_value\n",
        "\n",
        "    def _grow_tree(self, X, g, h, depth):\n",
        "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feature_idx, threshold, gain = self._find_best_split(X, g, h)\n",
        "\n",
        "        if feature_idx is None or gain <= 0:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        left_indices = X[:, feature_idx] < threshold\n",
        "        right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "        left_child = self._grow_tree(X[left_indices], g[left_indices], h[left_indices], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_indices], g[right_indices], h[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature_idx=feature_idx, threshold=threshold, left=left_child, right=right_child, gain=gain)\n",
        "\n",
        "    def fit(self, X, g, h):\n",
        "        self.root = self._grow_tree(X, g, h, depth=0)\n",
        "        print(\"Decision tree fitted with gradients and Hessians.\")\n",
        "\n",
        "    # (Placeholder for future methods: predict_one_sample, predict)\n",
        "\n",
        "print(\"DecisionTree class updated with fit method.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class updated with fit method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f64c258",
        "outputId": "a2043e7d-4376-4a98-db9e-eea43022334d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            return None, None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_gain <= 0 or np.isinf(best_gain):\n",
        "            return None, None, None, None\n",
        "\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    def _calculate_leaf_value(self, g, h):\n",
        "        G = np.sum(g)\n",
        "        H = np.sum(h)\n",
        "\n",
        "        if self.reg_alpha > 0:\n",
        "            if G > self.reg_alpha:\n",
        "                G -= self.reg_alpha\n",
        "            elif G < -self.reg_alpha:\n",
        "                G += self.reg_alpha\n",
        "            else:\n",
        "                G = 0\n",
        "\n",
        "        if (H + self.reg_lambda) > 0:\n",
        "            leaf_value = -G / (H + self.reg_lambda)\n",
        "        else:\n",
        "            leaf_value = 0\n",
        "\n",
        "        return leaf_value\n",
        "\n",
        "    def _grow_tree(self, X, g, h, depth):\n",
        "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feature_idx, threshold, gain = self._find_best_split(X, g, h)\n",
        "\n",
        "        if feature_idx is None or gain <= 0:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        left_indices = X[:, feature_idx] < threshold\n",
        "        right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "        left_child = self._grow_tree(X[left_indices], g[left_indices], h[left_indices], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_indices], g[right_indices], h[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature_idx=feature_idx, threshold=threshold, left=left_child, right=right_child, gain=gain)\n",
        "\n",
        "    def fit(self, X, g, h):\n",
        "        self.root = self._grow_tree(X, g, h, depth=0)\n",
        "        print(\"Decision tree fitted with gradients and Hessians.\")\n",
        "\n",
        "    def predict_one_sample(self, x):\n",
        "        node = self.root\n",
        "        while node.value is None: # While it's not a leaf node\n",
        "            if x[node.feature_idx] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.value\n",
        "\n",
        "    # (Placeholder for future method: predict)\n",
        "\n",
        "print(\"DecisionTree class updated with predict_one_sample method.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class updated with predict_one_sample method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "996cbe75",
        "outputId": "88d3b7ed-d6fd-40f1-9402-3dff4cb0edd7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            return None, None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_gain <= 0 or np.isinf(best_gain):\n",
        "            return None, None, None, None\n",
        "\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    def _calculate_leaf_value(self, g, h):\n",
        "        G = np.sum(g)\n",
        "        H = np.sum(h)\n",
        "\n",
        "        if self.reg_alpha > 0:\n",
        "            if G > self.reg_alpha:\n",
        "                G -= self.reg_alpha\n",
        "            elif G < -self.reg_alpha:\n",
        "                G += self.reg_alpha\n",
        "            else:\n",
        "                G = 0\n",
        "\n",
        "        if (H + self.reg_lambda) > 0:\n",
        "            leaf_value = -G / (H + self.reg_lambda)\n",
        "        else:\n",
        "            leaf_value = 0\n",
        "\n",
        "        return leaf_value\n",
        "\n",
        "    def _grow_tree(self, X, g, h, depth):\n",
        "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feature_idx, threshold, gain = self._find_best_split(X, g, h)\n",
        "\n",
        "        if feature_idx is None or gain <= 0:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        left_indices = X[:, feature_idx] < threshold\n",
        "        right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "        left_child = self._grow_tree(X[left_indices], g[left_indices], h[left_indices], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_indices], g[right_indices], h[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature_idx=feature_idx, threshold=threshold, left=left_child, right=right_child, gain=gain)\n",
        "\n",
        "    def fit(self, X, g, h):\n",
        "        self.root = self._grow_tree(X, g, h, depth=0)\n",
        "        # print(\"Decision tree fitted with gradients and Hessians.\") # Removed print to avoid clutter if many trees\n",
        "\n",
        "    def predict_one_sample(self, x):\n",
        "        node = self.root\n",
        "        while node.value is None: # While it's not a leaf node\n",
        "            if x[node.feature_idx] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_one_sample(x) for x in X])\n",
        "\n",
        "print(\"DecisionTree class updated with predict method.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTree class updated with predict method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c912a7b7"
      },
      "source": [
        "## Implement XGBoost Class: Initialization and Loss Function\n",
        "\n",
        "Define the `XGBoost` class, including its constructor to initialize parameters like the number of estimators and learning rate, and methods to compute gradients and Hessians for multiclass classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a9409cc",
        "outputId": "bca054bd-aef0-4a96-9833-2a911f492bbb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0, num_classes=10):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.num_classes = num_classes\n",
        "        self.models = [] # List to store decision trees for each class\n",
        "        # Initial prediction (before any trees are added) is log-odds ratio, usually initialized to 0 for all classes\n",
        "        # For multiclass, this will be an array of zeros for each class.\n",
        "        self.initial_prediction = np.zeros(self.num_classes)\n",
        "\n",
        "    def softmax(self, scores):\n",
        "        # Subtract max for numerical stability\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def _get_gradients_hessians(self, y_true, y_pred_proba):\n",
        "        # For multiclass logistic regression (softmax loss)\n",
        "        # y_true needs to be one-hot encoded\n",
        "        num_samples = y_true.shape[0]\n",
        "\n",
        "        # Gradients (first-order derivatives)\n",
        "        # g_k = y_pred_proba_k - y_true_k\n",
        "        gradients = y_pred_proba - y_true\n",
        "\n",
        "        # Hessians (second-order derivatives)\n",
        "        # h_k = y_pred_proba_k * (1 - y_pred_proba_k) (for binary)\n",
        "        # For multiclass, diagonal hessians are p_k * (1 - p_k)\n",
        "        # Off-diagonal are -p_j * p_k\n",
        "        # XGBoost typically uses diagonal Hessians for simplicity and efficiency\n",
        "        # h_k = p_k * (1 - p_k) for each class k\n",
        "        hessians = y_pred_proba * (1 - y_pred_proba)\n",
        "\n",
        "        return gradients, hessians\n",
        "\n",
        "print(\"XGBoost class initialized with constructor, softmax, and _get_gradients_hessians methods.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost class initialized with constructor, softmax, and _get_gradients_hessians methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dc25a16",
        "outputId": "ebfc763d-045d-4cd1-e100-22098d65664d"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0, num_classes=10):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.num_classes = num_classes\n",
        "        self.models = [] # List to store lists of decision trees, one list per class\n",
        "        self.initial_prediction = np.zeros(self.num_classes)\n",
        "\n",
        "    def softmax(self, scores):\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def _get_gradients_hessians(self, y_true_one_hot, y_pred_proba):\n",
        "        gradients = y_pred_proba - y_true_one_hot\n",
        "        # For multiclass, diagonal hessians are p_k * (1 - p_k)\n",
        "        # Using an approximation or more simply, for logistic regression,\n",
        "        # H_ii = p_i * (1 - p_i). XGBoost often uses this for simplicity.\n",
        "        hessians = y_pred_proba * (1 - y_pred_proba)\n",
        "        return gradients, hessians\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Convert y to one-hot encoding for multiclass loss calculation\n",
        "        y_true_one_hot = np.zeros((y.size, self.num_classes))\n",
        "        y_true_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "        # Initialize raw predictions (logits) to zeros\n",
        "        # shape: (num_samples, num_classes)\n",
        "        current_raw_predictions = np.zeros((X.shape[0], self.num_classes))\n",
        "\n",
        "        # Initialize models list, one list of trees per class\n",
        "        self.models = [[] for _ in range(self.num_classes)]\n",
        "\n",
        "        print(\"Starting XGBoost training...\")\n",
        "        for estimator_idx in range(self.n_estimators):\n",
        "            # Compute current probabilities\n",
        "            y_pred_proba = self.softmax(current_raw_predictions)\n",
        "\n",
        "            # Compute gradients and Hessians for all classes\n",
        "            gradients, hessians = self._get_gradients_hessians(y_true_one_hot, y_pred_proba)\n",
        "\n",
        "            for class_idx in range(self.num_classes):\n",
        "                # Create and train a DecisionTree for the current class using its specific\n",
        "                # gradients and Hessians\n",
        "                tree = DecisionTree(\n",
        "                    max_depth=self.max_depth,\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    reg_lambda=self.reg_lambda,\n",
        "                    reg_alpha=self.reg_alpha\n",
        "                )\n",
        "                # Pass gradients and Hessians specific to this class\n",
        "                tree.fit(X, gradients[:, class_idx], hessians[:, class_idx])\n",
        "                self.models[class_idx].append(tree)\n",
        "\n",
        "                # Update raw predictions for the current class using the new tree's predictions\n",
        "                # and the learning rate (shrinkage)\n",
        "                current_raw_predictions[:, class_idx] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            if (estimator_idx + 1) % 10 == 0 or estimator_idx == self.n_estimators - 1:\n",
        "                print(f\"  Estimator {estimator_idx + 1}/{self.n_estimators} trained.\")\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        print(f\"XGBoost training finished in {self.training_time:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    # (Placeholder for future method: predict)\n",
        "\n",
        "print(\"XGBoost class updated with fit method.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost class updated with fit method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6189065"
      },
      "source": [
        "## Consolidate All Model Results (Actual & Planned)\n",
        "\n",
        "Gather all accuracy and training time results for the models that have been executed successfully. Note the execution status and any errors encountered for XGBoost. Identify other models that were planned but not yet implemented. This consolidation will form the factual basis for the LaTeX report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4f765f4b",
        "outputId": "590a1e89-523b-4bc1-8024-7a8e729ae3da"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "# Load the training and validation datasets, skipping bad lines\n",
        "df_train = pd.read_csv('/content/MNIST_train.csv', on_bad_lines='skip')\n",
        "df_val = pd.read_csv('/content/MNIST_validation.csv', on_bad_lines='skip')\n",
        "\n",
        "X_train = df_train.drop(columns=['label', 'even']).values\n",
        "y_train = df_train['label'].values\n",
        "\n",
        "X_val = df_val.drop(columns=['label', 'even']).values\n",
        "y_val = df_val['label'].values\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "# Convert feature arrays to float and label arrays to int\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "y_train = y_train.astype('int32')\n",
        "y_val = y_val.astype('int32')\n",
        "\n",
        "# --- Node class definition ---\n",
        "class Node:\n",
        "    def __init__(self, feature_idx=None, threshold=None, value=None, left=None, right=None, gain=None):\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.gain = gain\n",
        "        self.value = value\n",
        "\n",
        "# --- DecisionTree class definition ---\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.root = None\n",
        "\n",
        "    def _calculate_gain(self, G_L, H_L, G_R, H_R):\n",
        "        def loss_term(G, H):\n",
        "            return -0.5 * ((G**2) / (H + self.reg_lambda)) if (H + self.reg_lambda) > 0 else 0\n",
        "\n",
        "        gain = loss_term(G_L, H_L) + loss_term(G_R, H_R) - loss_term(G_L + G_R, H_L + H_R)\n",
        "        return gain\n",
        "\n",
        "    def _find_best_split(self, X, g, h):\n",
        "        best_gain = -np.inf\n",
        "        best_feature_idx = None\n",
        "        best_threshold = None\n",
        "\n",
        "        if X.shape[0] < self.min_samples_split:\n",
        "            # Corrected: Return 3 None values if split is not possible\n",
        "            return None, None, None\n",
        "\n",
        "        for feature_idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, feature_idx])\n",
        "            if len(thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X[:, feature_idx] < threshold\n",
        "                right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "                if np.sum(left_indices) < self.min_samples_split or np.sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                G_L = np.sum(g[left_indices])\n",
        "                H_L = np.sum(h[left_indices])\n",
        "                G_R = np.sum(g[right_indices])\n",
        "                H_R = np.sum(h[right_indices])\n",
        "\n",
        "                current_gain = self._calculate_gain(G_L, H_L, G_R, H_R)\n",
        "\n",
        "                if current_gain > best_gain:\n",
        "                    best_gain = current_gain\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_gain <= 0 or np.isinf(best_gain):\n",
        "            # Corrected: Return 3 None values if no beneficial split is found\n",
        "            return None, None, None\n",
        "\n",
        "        # Corrected: Always return 3 values\n",
        "        return best_feature_idx, best_threshold, best_gain\n",
        "\n",
        "    def _calculate_leaf_value(self, g, h):\n",
        "        G = np.sum(g)\n",
        "        H = np.sum(h)\n",
        "\n",
        "        if self.reg_alpha > 0:\n",
        "            if G > self.reg_alpha:\n",
        "                G -= self.reg_alpha\n",
        "            elif G < -self.reg_alpha:\n",
        "                G += self.reg_alpha\n",
        "            else:\n",
        "                G = 0\n",
        "\n",
        "        if (H + self.reg_lambda) > 0:\n",
        "            leaf_value = -G / (H + self.reg_lambda)\n",
        "        else:\n",
        "            leaf_value = 0\n",
        "\n",
        "        return leaf_value\n",
        "\n",
        "    def _grow_tree(self, X, g, h, depth):\n",
        "        if depth >= self.max_depth or X.shape[0] < self.min_samples_split:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        # Corrected: Expect 3 values from _find_best_split\n",
        "        feature_idx, threshold, gain = self._find_best_split(X, g, h)\n",
        "\n",
        "        if feature_idx is None or gain <= 0:\n",
        "            leaf_value = self._calculate_leaf_value(g, h)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        left_indices = X[:, feature_idx] < threshold\n",
        "        right_indices = X[:, feature_idx] >= threshold\n",
        "\n",
        "        left_child = self._grow_tree(X[left_indices], g[left_indices], h[left_indices], depth + 1)\n",
        "        right_child = self._grow_tree(X[right_indices], g[right_indices], h[right_indices], depth + 1)\n",
        "\n",
        "        return Node(feature_idx=feature_idx, threshold=threshold, left=left_child, right=right_child, gain=gain)\n",
        "\n",
        "    def fit(self, X, g, h):\n",
        "        self.root = self._grow_tree(X, g, h, depth=0)\n",
        "\n",
        "    def predict_one_sample(self, x):\n",
        "        node = self.root\n",
        "        while node.value is None:\n",
        "            if node.feature_idx is None: # Added this check to prevent infinite loop for malformed trees\n",
        "                return 0 # Or raise an error\n",
        "            if x[node.feature_idx] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_one_sample(x) for x in X])\n",
        "\n",
        "# --- XGBoost class definition ---\n",
        "class XGBoost:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=5, min_samples_split=2, reg_lambda=1.0, reg_alpha=0.0, num_classes=10):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.reg_alpha = reg_alpha\n",
        "        self.num_classes = num_classes\n",
        "        self.models = [] # List to store lists of decision trees, one list per class\n",
        "        self.initial_prediction = np.zeros(self.num_classes)\n",
        "        self.training_time = 0.0\n",
        "\n",
        "    def softmax(self, scores):\n",
        "        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
        "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "    def _get_gradients_hessians(self, y_true_one_hot, y_pred_proba):\n",
        "        gradients = y_pred_proba - y_true_one_hot\n",
        "        hessians = y_pred_proba * (1 - y_pred_proba)\n",
        "        return gradients, hessians\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        start_time = time.time()\n",
        "\n",
        "        y_true_one_hot = np.zeros((y.size, self.num_classes))\n",
        "        y_true_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "        current_raw_predictions = np.zeros((X.shape[0], self.num_classes))\n",
        "\n",
        "        self.models = [[] for _ in range(self.num_classes)]\n",
        "\n",
        "        print(\"Starting XGBoost training...\")\n",
        "        for estimator_idx in range(self.n_estimators):\n",
        "            y_pred_proba = self.softmax(current_raw_predictions)\n",
        "            gradients, hessians = self._get_gradients_hessians(y_true_one_hot, y_pred_proba)\n",
        "\n",
        "            for class_idx in range(self.num_classes):\n",
        "                tree = DecisionTree(\n",
        "                    max_depth=self.max_depth,\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    reg_lambda=self.reg_lambda,\n",
        "                    reg_alpha=self.reg_alpha\n",
        "                )\n",
        "                tree.fit(X, gradients[:, class_idx], hessians[:, class_idx])\n",
        "                self.models[class_idx].append(tree)\n",
        "                current_raw_predictions[:, class_idx] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "            if (estimator_idx + 1) % 10 == 0 or estimator_idx == self.n_estimators - 1:\n",
        "                print(f\"  Estimator {estimator_idx + 1}/{self.n_estimators} trained.\")\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        print(f\"XGBoost training finished in {self.training_time:.2f} seconds.\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        raw_predictions = np.zeros((X.shape[0], self.num_classes))\n",
        "\n",
        "        for class_idx in range(self.num_classes):\n",
        "            for tree in self.models[class_idx]:\n",
        "                raw_predictions[:, class_idx] += self.learning_rate * tree.predict(X)\n",
        "\n",
        "        return self.softmax(raw_predictions)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.argmax(probabilities, axis=1)\n",
        "\n",
        "\n",
        "# Create an instance of the XGBoost model\n",
        "num_classes_mnist = len(np.unique(y_train))\n",
        "xgb_model = XGBoost(\n",
        "    n_estimators=50,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    min_samples_split=2,\n",
        "    reg_lambda=1.0,\n",
        "    reg_alpha=0.0,\n",
        "    num_classes=num_classes_mnist\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "print(\"Making predictions on the validation set using XGBoost...\")\n",
        "prediction_start_time = time.time()\n",
        "y_pred_val_xgb = xgb_model.predict(X_val)\n",
        "xgb_prediction_time = time.time() - prediction_start_time\n",
        "\n",
        "# Evaluate the model\n",
        "xgb_accuracy = np.mean(y_val == y_pred_val_xgb)\n",
        "\n",
        "print(f\"\\nXGBoost Validation Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Training Time: {xgb_model.training_time:.2f} seconds.\")\n",
        "print(f\"XGBoost Prediction Time: {xgb_prediction_time:.2f} seconds.\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting XGBoost training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;31m# Make predictions on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0mreg_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 )\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessians\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mcurrent_raw_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, g, h)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_one_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, g, h, depth)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mright_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, g, h, depth)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mright_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mleft_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mright_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grow_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36m_grow_tree\u001b[0;34m(self, X, g, h, depth)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Corrected: Expect 3 values from _find_best_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mfeature_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_best_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1436537746.py\u001b[0m in \u001b[0;36m_find_best_split\u001b[0;34m(self, X, g, h)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mright_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[0m\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}